# -*- coding: utf-8 -*-
"""DAY-6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11zaFnTg77D_Xd5Q_zJfP0L1hh3reCMap
"""

import numpy as np        # For numerical operations and arrays
import pandas as pd         # For data loading, manipulation, and analysis
import matplotlib.pyplot as  plt # for data visualzing
import seaborn as sns        # For advanced data visualization with better styling

data=pd.read_csv("/content/Cancer_Data.csv") # Import dataset
data # display the data

data.isnull() # displays whether the dataset contains null values or not

data.describe() # displays the Summary statistics for numerical columns

data.info() # displays the Summary of data types and non-null values

from sklearn.preprocessing import StandardScaler
x_scaled= data[['texture_mean']]  # Drop 'id'
scaler = StandardScaler()
x=scaler.fit_transform(x_scaled)

y=data['diagnosis'] #assign output to y
y.head() # displaying the top 5 values of y

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3) # here we are taking the test size as we want and the dataset are into 2 types . train and test dataset and we wil take test size as 30%

x_train # displaying the x_train (the data is randomly gathered)

y_train # displaying the output of x_train

from sklearn.neighbors import KNeighborsClassifier # importing the KNeighborsClassifier model

model=KNeighborsClassifier()

model.fit(x_train,y_train) # training the model by giving training data to find out relationship between input and output

model.predict(x_test) # giving the testing data to model how the model is predicting

y_test # displaying the y_test to observe the actual values versus model predicted values

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report # importing the evaluation metrics for classification type

accu=accuracy_score(y_test,model.predict(x_test)) # performing the accuracy of model
confuma=confusion_matrix(y_test,model.predict(x_test)) # displaying the confusion matrix
classre=classification_report(y_test,model.predict(x_test)) # displaying the classification report

print(accu)
print(confuma)
print(classre)

"""fine tuning of hyperparameter based on k value"""

alg=KNeighborsClassifier(n_neighbors=7) # fine tuning of one of the hyperparameters(k,dist) here we are taking K(here we are fine tuning manually)

alg.fit(x_train,y_train)

alg.predict(x_test)

y_test

accu=accuracy_score(y_test,alg.predict(x_test))
conf=confusion_matrix(y_test,alg.predict(x_test))
clas=classification_report(y_test,alg.predict(x_test))

print(accu)
print(conf)
print(clas)

from sklearn.model_selection import GridSearchCV # importing the GridSearchCV (fine tuning the hyperparameters automatically)

# here we are giving 3 hyper parameters and taking it in he form of list
n_neighbors_value=[1,3,5,7]
algorithm_value=['auto','ball_tree','kd_tree','brute']
p_value=[1,2,3,4]

param=dict(n_neighbors=n_neighbors_value,algorithm=algorithm_value,p=p_value)

g=GridSearchCV(model,param,scoring="accuracy",cv=5) # assiging parameters to the f=grudsearch cv

g # accessing the or running the grid search cv

grid=g.fit(x_train,y_train)  # fitting the training data

grid.best_params_ # giving the best parameters

grid.best_score_ # giving the best score of the model

# Create a range of values from the min to max of x
x_range = np.linspace(x_scaled.min() - 1, x_scaled.max() + 1, 300).reshape(-1, 1)
y_pred_range = grid.predict(x_range)

# Plot the decision boundary
plt.figure(figsize=(10, 5))
plt.scatter(x_scaled, y.map({'M': 1, 'B': 0}), c=y.map({'M': 'red', 'B': 'blue'}), label="Data points")
plt.plot(x_range, y_pred_range, color='green', linewidth=2, label="Decision Boundary")
plt.title("KNN Decision Boundary (1D) for texture_mean")
plt.xlabel("Normalized Texture Mean")
plt.ylabel("Diagnosis (M=1, B=0)")
plt.legend()
plt.grid(True)
plt.show()